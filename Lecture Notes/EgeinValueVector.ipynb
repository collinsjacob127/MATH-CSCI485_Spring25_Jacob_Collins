{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5532b09-fed9-41e2-b5cd-3d5eb89feba5",
   "metadata": {},
   "source": [
    "## Eigenvectors and Eigenvalues\n",
    "---\n",
    "### Theory  \n",
    "Eigenvectors and eigenvalues arise in **linear transformations**. Suppose we have an $ n \\times n $ matrix $ A $. An **eigenvector** $ v $ and **eigenvalue** $ \\lambda $ satisfy:  \n",
    "\n",
    "> \\$\n",
    "A v = \\lambda v\n",
    "\\$\n",
    "\n",
    "This equation means that applying $ A $ to $ v $ **scales** $ v $ by $ \\lambda $, without changing its direction.  \n",
    "\n",
    "ðŸ‘‰ **Key Insight**: Eigenvectors reveal fundamental directions along which a transformation acts **as simple scaling**, rather than distorting or rotating the space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Are Eigenvectors Orthogonal?**  \n",
    "Eigenvectors **are not always orthogonal**, but in the case of **symmetric matrices** ($ A = A^T $), they **must** be orthogonal.  \n",
    "\n",
    "#### **Mathematical Proof (Sketch)**\n",
    "For a symmetric matrix $ A $, let $ v_1 $ and $ v_2 $ be eigenvectors with different eigenvalues $ \\lambda_1 $ and $ \\lambda_2 $:\n",
    "\n",
    "> \\$\n",
    "A v_1 = \\lambda_1 v_1, \\quad A v_2 = \\lambda_2 v_2\n",
    "\\$\n",
    "\n",
    "Taking the dot product with $ v_2 $:\n",
    "\n",
    "> \\$\n",
    "v_2^T A v_1 = \\lambda_1 v_2^T v_1\n",
    "\\$\n",
    "\n",
    "Since $ A $ is symmetric, we can switch the order:\n",
    "\n",
    "> \\$\n",
    "(A v_2)^T v_1 = \\lambda_2 v_2^T v_1\n",
    "\\$\n",
    "\n",
    "Since the left-hand sides are equal:\n",
    "\n",
    "> \\$\n",
    "\\lambda_1 v_2^T v_1 = \\lambda_2 v_2^T v_1\n",
    "\\$\n",
    "\n",
    "If $ \\lambda_1 \\neq \\lambda_2 $, then **$ v_2^T v_1 = 0 $**, meaning $ v_1 $ and $ v_2 $ are **orthogonal**.\n",
    "\n",
    "ðŸ‘‰ **Key Insight**:  \n",
    "- Symmetric matrices describe **many real-world processes**, making eigenvectors orthogonal in those cases.\n",
    "- For **non-symmetric** matrices, eigenvectors may not be orthogonal, but they can often still form a useful basis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-Life Meaning of Eigenvalues**  \n",
    "Eigenvalues tell us how much the transformation **scales** each eigenvector direction. Some real-world interpretations:\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**\n",
    "   - The **largest eigenvalue** corresponds to the **most important direction** (principal component) in a dataset.\n",
    "   - The **smallest eigenvalues** correspond to **least important variations**.\n",
    "\n",
    "2. **Vibrations in Physics & Engineering**\n",
    "   - In structural engineering, eigenvalues represent **natural frequencies** of vibration.  \n",
    "   - A **bridge** or **building** has certain modes that oscillate at eigenvalue-determined frequencies.\n",
    "\n",
    "3. **Markov Chains & Stability**\n",
    "   - The **largest eigenvalue** of a transition matrix determines whether a system **stabilizes** or **diverges**.\n",
    "   - For example, in Googleâ€™s **PageRank algorithm**, the eigenvector of a probability matrix gives the **long-term importance** of web pages.\n",
    "\n",
    "4. **Quantum Mechanics**\n",
    "   - In quantum physics, **observable quantities (energy, momentum, etc.) correspond to eigenvalues** of operators.\n",
    "\n",
    "ðŸ‘‰ **Key Insight**:  \n",
    "- **Large eigenvalues** â†’ Dominant effects in the system (e.g., strongest principal components, highest vibration modes).  \n",
    "- **Small eigenvalues** â†’ Less significant effects (e.g., noise, negligible movement).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**  \n",
    "- **Eigenvectors**: Show **invariant directions** in transformations.\n",
    "- **Orthogonality**: Holds for **symmetric matrices**, simplifying computations.\n",
    "- **Eigenvalues**: Quantify the **effect of transformation**, with interpretations in data science, physics, and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c68ba-30e1-4658-a317-b601a32ab06c",
   "metadata": {},
   "source": [
    "### Numerical Example\n",
    "---\n",
    "Letâ€™s consider a simple **2D transformation** using a **symmetric** matrix:\n",
    "\n",
    "> \\$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "4 & 2 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix}\n",
    "\\$\n",
    "\n",
    "#### **Step 1: Compute Eigenvalues**\n",
    "We solve the **characteristic equation**:\n",
    "\n",
    "> \\$\n",
    "\\det(A - \\lambda I) = 0\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "\\begin{vmatrix}\n",
    "4 - \\lambda & 2 \\\\\n",
    "2 & 3 - \\lambda\n",
    "\\end{vmatrix} = 0\n",
    "\\$\n",
    "\n",
    "Expanding the determinant:\n",
    "\n",
    "> \\$\n",
    "(4 - \\lambda)(3 - \\lambda) - (2)(2) = 0\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "12 - 4\\lambda - 3\\lambda + \\lambda^2 - 4 = 0\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "\\lambda^2 - 7\\lambda + 8 = 0\n",
    "\\$\n",
    "\n",
    "Factoring:\n",
    "\n",
    "> \\$\n",
    "(\\lambda - 4)(\\lambda - 3) = 0\n",
    "\\$\n",
    "\n",
    "So, the eigenvalues are:\n",
    "\n",
    "> \\$\n",
    "\\lambda_1 = 4, \\quad \\lambda_2 = 3\n",
    "\\$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Compute Eigenvectors**\n",
    "For each eigenvalue $ \\lambda $, we solve $ (A - \\lambda I)v = 0 $, to find out\n",
    "> $\n",
    "v =\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "##### **For $ \\lambda_1 = 4 $:**\n",
    "\n",
    "> $\n",
    "\\begin{bmatrix}\n",
    "4 - 4 & 2 \\\\\n",
    "2 & 3 - 4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0 & 2 \\\\\n",
    "2 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "From the first row:\n",
    "\n",
    "> \\$\n",
    "2y = 0 \\Rightarrow y = 0\n",
    "\\$\n",
    "\n",
    "From the second row:\n",
    "\n",
    "> \\$\n",
    "2x - y = 0 \\Rightarrow 2x = y \\Rightarrow 2x = 0 \\Rightarrow x = 1\n",
    "\\$\n",
    "\n",
    "Eigenvector for $ \\lambda_1 = 4 $:\n",
    "\n",
    "> $\n",
    "v_1 =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "##### **For $ \\lambda_2 = 3 $:**\n",
    "\n",
    "> $\n",
    "\\begin{bmatrix}\n",
    "4 - 3 & 2 \\\\\n",
    "2 & 3 - 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 0\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "From the first row:\n",
    "\n",
    "> \\$\n",
    "x + 2y = 0 \\Rightarrow x = -2y\n",
    "\\$\n",
    "\n",
    "Choosing $ y = 1 $, we get $ x = -2 $.\n",
    "\n",
    "Eigenvector for $ \\lambda_2 = 3 $:\n",
    "\n",
    "> \\$\n",
    "v_2 =\n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Verify Orthogonality**\n",
    "The dot product:\n",
    "\n",
    "> \\$\n",
    "v_1^T v_2 =\n",
    "\\begin{bmatrix} 1 & 1 \\end{bmatrix}\n",
    "\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\n",
    "= (1)(-2) + (1)(1) = -2 + 1 = 0\n",
    "\\$\n",
    "\n",
    "Since the dot product is **zero**, the eigenvectors are **orthogonal**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation in PCA**\n",
    "\n",
    "1. **Eigenvalues $ \\lambda_1 = 4, \\lambda_2 = 3 $ indicate the \"strength\" of each eigenvector direction**.\n",
    "   - The eigenvector $ v_1 = [1,1] $ represents the **dominant** direction (largest variance).\n",
    "   - The eigenvector $ v_2 = [-2,1] $ represents the **secondary** direction (less variance).\n",
    "\n",
    "   \n",
    "2. **In PCA (Principal Component Analysis)**:\n",
    "   - If this matrix represents a **covariance matrix**, the **largest eigenvalue** tells us the **strongest correlation direction** in the dataset.\n",
    "   - Data points are most spread along $ v_1 $ (strongest principal component).\n",
    "\n",
    "3. **In Physics (Vibration Modes)**:\n",
    "   - These eigenvectors would correspond to **modes of oscillation** in a system.\n",
    "   - $ v_1 $ could be a major vibration mode, and $ v_2 $ a secondary one.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Insights**\n",
    "- **Eigenvalues measure \"importance\"** (variance in PCA, stability in Markov chains, frequency in vibrations).\n",
    "- **Eigenvectors are orthogonal** in symmetric matrices, making them ideal for decomposition.\n",
    "- **In PCA, eigenvectors define principal components, capturing most of the data variation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
