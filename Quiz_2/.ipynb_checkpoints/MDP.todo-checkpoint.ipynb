{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left;display:block} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left;display:block} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4002348",
   "metadata": {},
   "source": [
    "# HMM Forward / Backtracing Table - Weather\n",
    "\n",
    "Transition Probabilities: P(To|From)\n",
    "\n",
    "| From $\\rightarrow$ To  | Sunny | Cloudy | Rainy |\n",
    "|--------|------------|------------|------------|\n",
    "| Sunny    | 0.33 | 0.67 | 0.00 |\n",
    "| Cloudy   | 0.33 | 0.00 | 0.67 |\n",
    "| Rainy    | 0.33 | 0.33 | 0.33 |\n",
    "\n",
    "Emission Probabilities:\n",
    "P(Behavior|Weather)\n",
    "\n",
    "| Weather $\\rightarrow$ Behavior  | Walk | Umbrella |\n",
    "|--------|------------|------------|\n",
    "| Sunny    | 4/4 = 1.0 | 0/3 = 0 |\n",
    "| Cloudy   | 2/3 $\\approx$ 0.67 | 1/3 $\\approx$ 0.33 |\n",
    "| Rainy    | 1/3 $\\approx$ 0.33 | 2/3 $\\approx$ 0.67 |\n",
    "\n",
    "![](forward_back_tracing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "----\n",
    "\n",
    "**Value Iteration Process with Policy Changes in MDP**\n",
    "\n",
    "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Defining the MDP Components**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "**Actions (A):**\n",
    "\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "**Transition Probabilities:**\n",
    "\n",
    "| Current State | Action | Next State Probabilities     |\n",
    "| ------------- | ------ | ---------------------------- |\n",
    "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- Low Wealth (L): -1\n",
    "- Medium Wealth (M): 3\n",
    "- High Wealth (H): 5\n",
    "\n",
    "**Discount Factor (γ):** 0.9\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6d6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 0.9\n",
      "\n",
      "State: L\n",
      "    Reward: -1\n",
      "    C: {'L': 0.8, 'M': 0.2}\n",
      "    A: {'L': 0.6, 'M': 0.4}\n",
      "State: M\n",
      "    Reward: 3\n",
      "    C: {'M': 0.7, 'H': 0.3}\n",
      "    A: {'M': 0.5, 'H': 0.5}\n",
      "State: H\n",
      "    Reward: 5\n",
      "    C: {'H': 0.9, 'M': 0.1}\n",
      "    A: {'H': 0.7, 'M': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# Define the MDP Tree\n",
    "mdp_tree = {\n",
    "    \"L\": {\n",
    "        \"Reward\": -1,\n",
    "        \"C\": { \"L\": 0.8, \"M\": 0.2 },\n",
    "        \"A\": { \"L\": 0.6, \"M\": 0.4 },\n",
    "    },\n",
    "    \"M\": {\n",
    "        \"Reward\": 3,\n",
    "        \"C\": { \"M\": 0.7, \"H\": 0.3 },\n",
    "        \"A\": { \"M\": 0.5, \"H\": 0.5 },\n",
    "    },\n",
    "    \"H\": {\n",
    "        \"Reward\": 5,\n",
    "        \"C\": { \"H\": 0.9, \"M\": 0.1 },\n",
    "        \"A\": { \"H\": 0.7, \"M\": 0.3 },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Verify that it matches the table\n",
    "def print_mdp(mdp):\n",
    "    for state, vals in mdp.items():\n",
    "        print(f'State: {state}')\n",
    "        print(f'    Reward: {vals[\"Reward\"]}')\n",
    "        for action in [\"C\", \"A\"]:\n",
    "            print(f'    {action}: ', end=\"\")\n",
    "            print(f'{vals[action]}')\n",
    "            if (sum(vals[action].values()) != 1):\n",
    "                print(\"    Bad probability sum\")\n",
    "\n",
    "gamma = 0.9\n",
    "print(f'gamma = {gamma}')\n",
    "print()\n",
    "print_mdp(mdp_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
   "metadata": {},
   "source": [
    "### **Step 2: Value Iteration Updates**\n",
    "\n",
    "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "Using Bellman’s equation:\n",
    "\n",
    ">$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cbe624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      "V_0(L) = 0.00\n",
      "V_0(M) = 0.00\n",
      "V_0(H) = 0.00\n",
      "Q(L, C) = -1.00\n",
      "Q(L, A) = -1.00\n",
      "Q(M, C) = 3.00\n",
      "Q(M, A) = 3.00\n",
      "Q(H, C) = 5.00\n",
      "Q(H, A) = 5.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tree: state -> {reward, choices -> { destination -> probability }  }\n",
    "\n",
    "# Values in each iteration\n",
    "def initialize_values(mdp):\n",
    "    V = {}\n",
    "    for state in mdp.keys():\n",
    "        V[state] = [0]\n",
    "    return V\n",
    "\n",
    "# Define Bellman's equation as a function\n",
    "def bellmans(mdp, state, step, values, min_printed=0, only_m=False):\n",
    "    actions = mdp[state]\n",
    "    # List to find max of; Q-score for each choice\n",
    "    opts = []\n",
    "    for choice, dests in {k: v for (k,v) in actions.items() if k != \"Reward\"}.items():\n",
    "        # Initialize with reward of current state\n",
    "        reward = actions[\"Reward\"]\n",
    "        tmp = 0\n",
    "        # Add reward of next states weighted by their probability\n",
    "        for dest, prob in dests.items():\n",
    "            tmp += prob * values[dest][step-1]\n",
    "        # Account for gamma\n",
    "        tmp *= gamma\n",
    "        reward += tmp\n",
    "        if (step > min_printed and (not only_m or state == \"M\")):\n",
    "            print(f'Q({state}, {choice}) = {reward:.2f}')\n",
    "        opts.append(reward)\n",
    "    # if (step >= min_printed):\n",
    "    #     print(f'V_{step}({state})={max(opts):.2f}')\n",
    "    return max(opts)\n",
    "\n",
    "def run_iterations(n_iter=5, min_printed=0, mdp_tree=mdp_tree, only_m=False):\n",
    "    V = initialize_values(mdp_tree)\n",
    "\n",
    "    for i in range(n_iter+1):\n",
    "        if (i >= min_printed):\n",
    "            print(f'Iteration {i}:')\n",
    "            for state in mdp_tree.keys():\n",
    "                print(f'V_{i}({state}) = {V[state][i]:.2f}')\n",
    "        for state in mdp_tree.keys():\n",
    "            V[state].append(bellmans(mdp_tree, state, i+1, V, min_printed=min_printed, only_m=only_m))\n",
    "\n",
    "        if (i >= min_printed):\n",
    "            print()\n",
    "\n",
    "run_iterations(n_iter=0, min_printed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b1b61b",
   "metadata": {},
   "source": [
    ">$\n",
    "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 1**\n",
    "\n",
    "> $\n",
    "Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.44 (should be -1.18)\n",
    "$\n",
    "\n",
    "> $\n",
    "Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.76 (should be -0.46)\n",
    "$\n",
    "\n",
    "Complete the rest...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5c2663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: Q(L,C) = -1.18\n",
      "Sanity check: Q(L,A) = -0.46\n",
      "Where did -1.44 and -.076 come from? That doesn't make sense to me.\n",
      "\n",
      "Iteration 1:\n",
      "V_1(L) = -1.00\n",
      "V_1(M) = 3.00\n",
      "V_1(H) = 5.00\n",
      "Q(L, C) = -1.18\n",
      "Q(L, A) = -0.46\n",
      "Q(M, C) = 6.24\n",
      "Q(M, A) = 6.60\n",
      "Q(H, C) = 9.32\n",
      "Q(H, A) = 8.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Sanity check: Q(L,C) = {-1 + 0.9 * (0.8 * (-1) + 0.2 * (3)):.2f}')\n",
    "print(f'Sanity check: Q(L,A) = {-1 + 0.9 * (0.6 * (-1) + 0.4 * (3)):.2f}')\n",
    "print(\"Where did -1.44 and -.076 come from? That doesn't make sense to me.\\n\")\n",
    "run_iterations(n_iter=1, min_printed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7eace",
   "metadata": {},
   "source": [
    "**Policy at Iteration 1:**\n",
    "- L → **Aggressive (A)**\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040d995",
   "metadata": {},
   "source": [
    "#### **Iteration 2**\n",
    "\n",
    "Updating $V_2(s)$:\n",
    "\n",
    ">$\n",
    "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c564ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking V_2(L) = -0.46\n",
      "Checking V_2(H) = 9.32\n"
     ]
    }
   ],
   "source": [
    "print(f'Checking V_2(L) = {max([(-1 + 0.9 * (0.8 * (-1) + 0.2 * (3))), (-1 + 0.9 * (0.6 * (-1) + 0.4 * (3)))]):.2f}')\n",
    "print(f'Checking V_2(H) = {max([(5 + 0.9 * (0.9 * (5) + 0.1 * (3))), (5 + 0.9 * (0.7 * (5) + 0.3 * (3)))]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111dab85",
   "metadata": {},
   "source": [
    "#### **Policy Evaluation after Iteration 2**\n",
    "\n",
    "Complete the iteration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30212bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2:\n",
      "V_2(L) = -0.46\n",
      "V_2(M) = 6.60\n",
      "V_2(H) = 9.32\n",
      "Q(L, C) = -0.14\n",
      "Q(L, A) = 1.13\n",
      "Q(M, C) = 9.67\n",
      "Q(M, A) = 10.16\n",
      "Q(H, C) = 13.14\n",
      "Q(H, A) = 12.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_iterations(n_iter = 2, min_printed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569adba7",
   "metadata": {},
   "source": [
    "**Policy at Iteration 2:**\n",
    "- L → Aggressive (A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c65147",
   "metadata": {},
   "source": [
    "\n",
    "#### **Iteration 3**\n",
    "\n",
    "Updating $V_3(s)$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b37c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3:\n",
      "V_3(L) = 1.13\n",
      "V_3(M) = 10.16\n",
      "V_3(H) = 13.14\n",
      "Q(L, C) = 1.64\n",
      "Q(L, A) = 3.27\n",
      "Q(M, C) = 12.95\n",
      "Q(M, A) = 13.49\n",
      "Q(H, C) = 16.56\n",
      "Q(H, A) = 16.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_iterations(n_iter = 3, min_printed=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f152abeb",
   "metadata": {},
   "source": [
    "#### **Policy Change Analysis**\n",
    "\n",
    "From **Iteration 2 to Iteration 3**, let’s check the action values to determine if the policy changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75ca6b",
   "metadata": {},
   "source": [
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "Q(L, C) = 1.64$\n",
    "\n",
    ">$\n",
    "Q(L, A) = 3.27$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "Q(M, C) = 12.95\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(M, A) = 13.49\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "Q(H, C) = 16.56\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(H, A) = 16.02\n",
    "$\n",
    "\n",
    "Compare $Q(L, A), Q(L, C)$ and $Q(H, C),  Q(H, A)$, decide the policy updates:\n",
    "\n",
    "- **Low Wealth (L)** → Aggressive (A)\n",
    "- **Medium Wealth (M)** → Aggressive (A)\n",
    "- **High Wealth (H)** → Conservative (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5930c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "V_1(L) = -1.00\n",
      "V_1(M) = 3.00\n",
      "V_1(H) = 5.00\n",
      "Q(L, C) = -1.18\n",
      "Q(L, A) = -0.46\n",
      "Q(M, C) = 6.24\n",
      "Q(M, A) = 6.60\n",
      "Q(H, C) = 9.32\n",
      "Q(H, A) = 8.96\n",
      "\n",
      "Iteration 2:\n",
      "V_2(L) = -0.46\n",
      "V_2(M) = 6.60\n",
      "V_2(H) = 9.32\n",
      "Q(L, C) = -0.14\n",
      "Q(L, A) = 1.13\n",
      "Q(M, C) = 9.67\n",
      "Q(M, A) = 10.16\n",
      "Q(H, C) = 13.14\n",
      "Q(H, A) = 12.65\n",
      "\n",
      "Iteration 3:\n",
      "V_3(L) = 1.13\n",
      "V_3(M) = 10.16\n",
      "V_3(H) = 13.14\n",
      "Q(L, C) = 1.64\n",
      "Q(L, A) = 3.27\n",
      "Q(M, C) = 12.95\n",
      "Q(M, A) = 13.49\n",
      "Q(H, C) = 16.56\n",
      "Q(H, A) = 16.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_iterations(n_iter=3, min_printed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068",
   "metadata": {},
   "source": [
    "### Summary: Policy Evolution Over Iterations\n",
    "\n",
    "| State  | Iteration 1 | Iteration 2 | Iteration 3 |\n",
    "|--------|------------|------------|------------|\n",
    "| Low      | A          | A            | A          |\n",
    "| Medium   | A            | A            | A          |\n",
    "| High     | C          | C          | C          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0f285",
   "metadata": {},
   "source": [
    "I generalized the function for MDP to work with any set of states and choices, and the output is has effectively the same content as a detailed table would, so I'm not going to the trouble of hard-coding everything in a spreadsheet again, hope you don't mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11026c0e",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "\n",
    "These results make a lot of sense- There is no way for `L` to reach a state with lower reward, so it will always take the route with the highest chance of going to a state with increased reward. The same goes for state `M`. State `H` has the possibility of going to a state with lower reward, and as such chooses the route with the lowest probability of decreasing reward over time.\n",
    "\n",
    "To see actual policy evolution over iterations, we could change the mdp tree to give more risk to an aggressive strategy, and see what changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739f85f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "V_1(L) = -1.00\n",
      "V_1(M) = 3.00\n",
      "V_1(H) = 5.00\n",
      "V_1(S) = 2.00\n",
      "Q(M, C) = 5.88\n",
      "Q(M, A) = 6.24\n",
      "Q(M, S) = 4.81\n",
      "\n",
      "Iteration 2:\n",
      "V_2(L) = -0.46\n",
      "V_2(M) = 6.24\n",
      "V_2(H) = 9.32\n",
      "V_2(S) = 6.48\n",
      "Q(M, C) = 8.89\n",
      "Q(M, A) = 9.45\n",
      "Q(M, S) = 8.83\n",
      "\n",
      "Iteration 3:\n",
      "V_3(L) = 1.00\n",
      "V_3(M) = 9.45\n",
      "V_3(H) = 13.11\n",
      "V_3(S) = 10.36\n",
      "Q(M, C) = 11.83\n",
      "Q(M, A) = 12.49\n",
      "Q(M, S) = 12.32\n",
      "\n",
      "Iteration 4:\n",
      "V_4(L) = 2.94\n",
      "V_4(M) = 12.49\n",
      "V_4(H) = 16.47\n",
      "V_4(S) = 13.77\n",
      "Q(M, C) = 14.60\n",
      "Q(M, A) = 15.32\n",
      "Q(M, S) = 15.38\n",
      "\n",
      "Iteration 5:\n",
      "V_5(L) = 5.08\n",
      "V_5(M) = 15.38\n",
      "V_5(H) = 19.46\n",
      "V_5(S) = 16.79\n",
      "Q(M, C) = 17.21\n",
      "Q(M, A) = 17.94\n",
      "Q(M, S) = 18.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjusted_mdp_tree = {\n",
    "    \"L\": {\n",
    "        \"Reward\": -1,\n",
    "        \"C\": { \"L\": 0.8, \"M\": 0.2 },\n",
    "        \"A\": { \"L\": 0.6, \"M\": 0.4 },\n",
    "    },\n",
    "    \"M\": {\n",
    "        \"Reward\": 3,\n",
    "        \"C\": { \"M\": 0.9, \"H\": 0.1 },\n",
    "        \"A\": { \"M\": 0.7, \"H\": 0.3 },\n",
    "        \"S\": { \"M\": 0.01, \"S\": 0.99 }\n",
    "    },\n",
    "    \"H\": {\n",
    "        \"Reward\": 5,\n",
    "        \"C\": { \"H\": 0.9, \"M\": 0.1 },\n",
    "        \"A\": { \"H\": 0.7, \"M\": 0.3 },\n",
    "    },\n",
    "    # Route that will not initially appear beneficial, but has high probability of leading to H.\n",
    "    \"S\": {\n",
    "        \"Reward\": 2,\n",
    "        \"C\": { \"M\": 0.01, \"H\": 0.99 },\n",
    "        \"A\": { \"L\": 0.3, \"M\": 0.7 },\n",
    "    },\n",
    "}\n",
    "# print_mdp(adjusted_mdp_tree)\n",
    "\n",
    "run_iterations(n_iter = 5, min_printed = 1, mdp_tree=adjusted_mdp_tree, only_m=True)\n",
    "# run_iterations(n_iter = 5, min_printed = 1, mdp_tree=adjusted_mdp_tree, only_m=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da657ba0",
   "metadata": {},
   "source": [
    "In this example, I have added an additional state `S`, which has a reward of 2 (lower than `R(M)`), but a high probability of reaching state `H`, which has the highest reward.\n",
    "\n",
    "This scenario successfully demonstrates the usefulness of iteration here, because state M changes from the aggressive strategy (iter 0-3), to the secret strategy at iteration 4.\n",
    "\n",
    "Iteration 3:\n",
    "- **Q(M, A) = 12.49**\n",
    "- Q(M, S) = 12.32\n",
    "\n",
    "Iteration 5:\n",
    "- Q(M, A) = 17.94\n",
    "- **Q(M, S) = 18.10**\n",
    "\n",
    "| State  | Iteration 3 | Iteration 4 | Iteration 5 |\n",
    "|--------|------------|------------|------------|\n",
    "| Low (L)     | A          | A            | A          |\n",
    "| **Medium (M)**   | **A**            | **S**            | **S**          |\n",
    "| High (H)     | C          | C          | C          |\n",
    "| Secret (S)     | C          | C          | C          |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
